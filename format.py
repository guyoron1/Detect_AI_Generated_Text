# To format data to competition format - including creation of prompts based on promptless ones
import json
import os
from transformers import pipeline
from tqdm import tqdm
import pandas as pd
import numpy as np

import fetch_data
GLOBAL_PIPE = pipeline("text2text-generation", model="google/flan-t5-large")

DATASET_NAME_TO_PATH = {
    #'daigt': './external_sources/daigt/train_v2_drcat_02.csv',
    #'persuade': './external_sources/persuade/persuade_corpus_2.0_train.csv',
    'fpe': './external_sources/fpe'
}

REQUIRED_COLS = (
    'prompt_name', # Identifier for prompt in plaintext.
    'prompt_text', # Actual prompt.
    'essay', # Essay writted by student / LLM.
    'is_prompt_llm_generated', # For datasets that lack prompts - whether the prompt was generated by us in reverse.
    'generated', # Classification label - 0 by student, 1 by LLM (target)
    )


def format_all_datasets(read_from_existing:str):
    if read_from_existing: #read_from_existing is path
        df = pd.read_csv(read_from_existing)
        return df
    all_dataframes = []
    for name, _ in DATASET_NAME_TO_PATH.items():
        df = format_dataset(name)
        all_dataframes.append(df)
    concatenated_df = pd.concat(all_dataframes, ignore_index=True)
    concatenated_df.reset_index(drop=True, inplace=True)
    return concatenated_df

def format_dataset(dataset_name):
    """
    Given dataset identifier return df with data as formatted for competition.
    """
    path = DATASET_NAME_TO_PATH[dataset_name]
    plain_df = dataset_to_pandas(dataset_name, path)
    df_with_prompts = add_prompts(plain_df) # Fill in prompts with LLM pipeline (if needed)
    return df_with_prompts


def dataset_to_pandas(dataset_name, path):
    # Divide into cases by datasets.
    if dataset_name == 'daigt':
        df = format_daigt_to_df(path)
    elif dataset_name == 'persuade':
        df = format_persuade_to_df(path)
    elif dataset_name == 'fpe':
        df = format_fpe_to_df(path)

    df = df.drop_duplicates(subset='essay_text')
    return df

def format_daigt_to_df(path):
    df = pd.read_csv(path)
    df = df.rename(columns={'text':'essay_text','label':'generated'})
    df = df[df['RDizzl3_seven'] == True] # Filtering present in Daigt2.0 that says whether the essay was written in response to an original prompt from the challange.
    df = df.drop(columns=['RDizzl3_seven','source'])
    mapping = fetch_data.map_prompt_name_to_prompt_text_persuade()
    df['prompt_text'] = df['prompt_name'].map(mapping)
    df['source'] = 'daigt'
    return df

def format_persuade_to_df(path):
    df = pd.read_csv(path)
    df = df.rename(columns={'full_text':'essay_text','assignment':'prompt_text'})
    df = df[['prompt_name','prompt_text','essay_text']]
    df['generated'] = 0
    df['source'] = 'persuade'
    return df

def format_fpe_to_df(path):
    dataframes = {}
    for file in os.listdir(path):
        file_path = os.path.join(path, file)
        df_name = os.path.splitext(file)[0]  # Use the filename (without extension) as the key
        if file.endswith(".csv"):
            if df_name == 't5_essays_processed' or df_name == 'mlm_essays_processed':
                dataframes[df_name] = pd.read_csv(file_path)
        elif file.endswith(".parquet") and not(file.startswith('cv')):
            df = pd.read_parquet(file_path)
            dataframes[df_name] = df

    # Format datasets
    for name, df in dataframes.items():
        if name == 't5_essays_processed':
            df = df[['essay_text']]
            df['generated'] = 1
        elif name == 'mlm_essays_processed':
            df = df.rename(columns={'prompt':'prompt_text'})
            df = df[['essay_text', 'prompt_text']]
            df['generated'] = 1
        elif name.startswith("fpe"):
            df = df[['essay_text']]
            df['generated'] = 0
        else:
            print('a')
        dataframes[name] = df
    result = pd.concat(
        [df.assign(source=key) for key, df in dataframes.items()],
        ignore_index=True,
        sort=False  # Align columns automatically, filling missing ones with NaN
    )
    return result


def add_prompts(df: pd.DataFrame, batch_size: int = 8):
    """
    Fills in missing 'prompt_text' in the DataFrame using an LLM pipeline in batches.

    Args:
        df (pd.DataFrame): DataFrame with 'essay_text' and optionally 'prompt_text'.
        batch_size (int): Number of rows to process in each batch.

    Returns:
        pd.DataFrame: Updated DataFrame with 'prompt_text' filled where missing.
    """
    # Determine rows that need filling
    fill_all = 'prompt_text' not in df.columns
    if fill_all:
        df['prompt_text'] = None  # Add the missing column

    # Identify rows where 'prompt_text' is NaN or fill_all is True
    rows_to_fill = df.index[df['prompt_text'].isna()] if not fill_all else df.index

    if rows_to_fill.empty:
        return df  # No missing prompts to fill

    # Process in batches
    for start_idx in tqdm(
            range(0, len(rows_to_fill), batch_size),
            desc="Filling prompts in batches",
            total=(len(rows_to_fill) + batch_size - 1) // batch_size,
    ):
        batch_indices = rows_to_fill[start_idx:start_idx + batch_size]
        batch_texts = df.loc[batch_indices, 'essay_text'].tolist()

        # Generate prompts for the batch
        generated_prompts = generate_prompts_for_texts(batch_texts, GLOBAL_PIPE, batch_size=batch_size)

        # Update the DataFrame with generated prompts
        df.loc[batch_indices, 'prompt_text'] = generated_prompts

    return df


def generate_prompts_for_texts(texts, pipe, max_input_tokens=512, batch_size=16):
    """
    Generates prompts for a batch of texts, ensuring only the middle text is truncated.

    Args:
        texts (list of str): List of input texts.
        pipe: Text-to-text generation pipeline.
        max_input_tokens (int): Maximum tokens for the input.
        batch_size (int): Batch size.

    Returns:
        list of str: List of generated prompts.
    """
    from transformers import AutoTokenizer
    import torch

    # Load the tokenizer
    tokenizer = pipe.tokenizer if hasattr(pipe, 'tokenizer') else AutoTokenizer.from_pretrained(pipe.model.name_or_path)

    prompt_prefix = "This is an essay written by a student in response to an assignment: \n"
    prompt_suffix = "\n Write the assignment that you think was given to the student, and phrase your answer as a question."

    # Tokenize the prefix and suffix separately
    prefix_ids = tokenizer(prompt_prefix, return_tensors="pt").input_ids[0]
    suffix_ids = tokenizer(prompt_suffix, return_tensors="pt").input_ids[0]
    available_tokens = max_input_tokens - (len(prefix_ids) + len(suffix_ids))

    truncated_inputs = []
    for text in texts:
        # Tokenize and truncate the middle text
        text_ids = tokenizer(text, truncation=True, max_length=available_tokens, return_tensors="pt").input_ids[0]
        # Reconstruct the input
        truncated_input_ids = torch.cat([prefix_ids, text_ids, suffix_ids], dim=0)
        truncated_input = tokenizer.decode(truncated_input_ids, skip_special_tokens=True)
        truncated_inputs.append(truncated_input)

    # Generate prompts in batch
    generated = pipe(
        truncated_inputs,
        num_return_sequences=1,
        do_sample=False,
        temperature=0.3,
        batch_size=batch_size,
    )

    return [g['generated_text'].strip() for g in generated]


def write_mistral_format(dataset: pd.DataFrame, output_file: str):
    """
    Writes the dataset into the Mistral format as a JSON file.

    Args:
        dataset (pd.DataFrame): The input dataset with columns
                                'prompt_text', 'essay_text', 'generated', and 'source'.
        output_file (str): Path to the output JSON file.
    """
    mistral_data = []

    for _, row in dataset.iterrows():
        # Determine the label based on the 'generated' column
        label = "LLM" if row['generated'] == 1 else "Human"

        # Construct the input string
        input_text = f"Prompt Text: {row['prompt_text']}. Essay Text: {row['essay_text']}"

        # Add the entry to the Mistral data list
        mistral_data.append({
            "input": input_text,
            "label": label
        })

    # Write the JSON data to the specified output file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(mistral_data, f, ensure_ascii=False, indent=4)



def sample_by_percentages(df: pd.DataFrame, percentages: dict) -> pd.DataFrame:
    """
    Samples the dataset based on the given percentages for each source.

    Args:
        df (pd.DataFrame): The input dataset with a 'source' column.
        percentages (dict): A dictionary mapping sources to their respective percentages (0-1).

    Returns:
        pd.DataFrame: A new DataFrame with the sampled data.
    """
    sampled_data = []

    for source, percentage in percentages.items():
        # Filter rows belonging to the current source
        source_df = df[df['source'] == source]

        # Calculate the number of samples
        num_samples = int(len(source_df) * percentage)

        # Sample the data
        sampled_df = source_df.sample(n=num_samples, random_state=42)  # Setting random_state for reproducibility

        # Add the sampled data to the list
        sampled_data.append(sampled_df)

    # Concatenate all sampled dataframes
    result_df = pd.concat(sampled_data, ignore_index=True)
    return result_df


if __name__ == '__main__':
    #df = format_all_datasets()
    data = {
        "prompt_text": [
            "What are the benefits of exercise?",
            "Describe the importance of technology in education."
        ],
        "essay_text": [
            "Exercise improves mental and physical health, helping people lead a balanced life.",
            "Technology enhances learning by providing access to resources and enabling remote education."
        ],
        "generated": [0, 1],  # 0 for Human, 1 for LLM
        "source": ["human_dataset", "llm_dataset"]
    }

    df = pd.DataFrame(data)
    write_mistral_format(df, 'output.json')